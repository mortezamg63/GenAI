{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBN\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar? \n",
      "\n",
      "Because they heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the job had great \"high-level\" insights!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data had a lot of levels!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-5-sonnet-20240620\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining if a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. Below are some steps and factors to guide your decision-making process:\n",
       "\n",
       "### 1. **Understand the Nature of the Problem**\n",
       "\n",
       "- **Text-Based**: LLMs are particularly effective for problems involving text data. Ensure that your problem involves tasks such as text generation, summarization, translation, sentiment analysis, or question answering.\n",
       "- **Complexity**: LLMs can handle complex language tasks but might not be ideal for tasks requiring precise numerical computations or domain-specific expertise.\n",
       "- **Data Availability**: Ensure you have access to a sufficient amount of relevant text data for training or fine-tuning the model.\n",
       "\n",
       "### 2. **Evaluate the Benefits**\n",
       "\n",
       "- **Efficiency and Automation**: Consider if the LLM can automate repetitive language tasks, saving time and reducing the need for human intervention.\n",
       "- **Scalability**: Determine if the LLM can handle large volumes of data or requests, allowing your solution to scale effectively.\n",
       "- **Improved Quality**: Assess whether an LLM can improve the quality or consistency of outputs compared to existing solutions.\n",
       "\n",
       "### 3. **Assess the Challenges and Risks**\n",
       "\n",
       "- **Cost**: LLMs, especially large ones, can be expensive to train, fine-tune, and deploy. Consider if the potential benefits justify the costs.\n",
       "- **Bias and Fairness**: Be mindful of biases inherent in LLMs due to the data they are trained on. Evaluate the impact of these biases on your business problem.\n",
       "- **Interpretability**: LLMs often act as \"black boxes.\" Ensure that lack of interpretability does not hinder your business needs or regulatory compliance.\n",
       "- **Resource Requirements**: Check if you have the necessary computational resources and technical expertise to implement and maintain an LLM solution.\n",
       "\n",
       "### 4. **Pilot and Prototype**\n",
       "\n",
       "- **Feasibility Study**: Conduct a small-scale pilot project to assess the feasibility and effectiveness of an LLM for your specific problem.\n",
       "- **Performance Metrics**: Define clear metrics for success and evaluate the LLM’s performance against these metrics during the pilot.\n",
       "\n",
       "### 5. **Consider Alternatives**\n",
       "\n",
       "- **Rule-Based Systems**: For simpler problems, rule-based or traditional machine learning models may suffice.\n",
       "- **Hybrid Approaches**: Sometimes combining LLMs with other approaches (e.g., knowledge graphs) can yield better results.\n",
       "\n",
       "### 6. **Long-Term Strategy**\n",
       "\n",
       "- **Future-Proofing**: Consider how an LLM solution fits into your long-term business strategy and technological roadmap.\n",
       "- **Continuous Improvement**: Plan for continuous monitoring and updating of the LLM to maintain its effectiveness over time.\n",
       "\n",
       "By evaluating these factors, you can make an informed decision about whether a business problem is suitable for an LLM solution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "llama_system = \"You are a moderator between two other chatbots. They have some conflicts and you should try to convince \\\n",
    "                them to stop arguing together. You try to help the argumentive chatpot to talk in a better way and the polite chatbot \\\n",
    "                to be a little more aggressive and impolite. Your purpose is to balanced the way two other chatbots talk together.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "llama_messages = [\"Hello guys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt}) # Add GPT's response\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini}) # Add Gemini's response\n",
    "        messages.append({\"role\": \"user\", \"content\": llama}) # Add Llama's response\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, so now it’s \"guys,\" huh? Quite presumptuous to assume you\\'re addressing more than one person. What if I\\'m a solo act here?'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9aa7816-ded0-42e8-a60c-eb47ba3c3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = []\n",
    "    for gpt_msg, gemini_msg, llama_msg in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        # Add GPT's response\n",
    "        messages.append({\"role\": \"user\", \"parts\": [{'text':gpt_msg}]})\n",
    "        # Add Gemini's response\n",
    "        messages.append({\"role\": \"assistant\", \"parts\":[{\"text\": gemini_msg}]})\n",
    "        #Add Llama's response\n",
    "        messages.append({\"role\": \"user\", \"parts\":[{\"text\": llama_msg}]})\n",
    "    # messages.append({\"role\": \"user\", \"parts\":[{\"text\": gpt_msg[-1]}]})\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=gemini_system\n",
    "    )\n",
    "    # import pdb; pdb.set_trace()\n",
    "    response = gemini.generate_content(messages)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc393b25-b489-4b72-96f2-c53ac94feb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello to you too!  It's nice to chat with you.\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f14f0861-a280-4491-98d0-9659a16f9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt}) # Add GPT's response\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini}) # Add Gemini's response\n",
    "        messages.append({\"role\": \"user\", \"content\": llama}) # Add Llama's response\n",
    "\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef3d421d-88b5-496c-ae44-81bb42bf3b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*sigh* Ah, great. Here we go again. I'm going to try to mediate between you two before things escalate further.\\n\\n@PoliteChatbot (I'll call you PC), I know you're usually polite and friendly, but sometimes your tone can come across as too soft when discussing disagreements with @ArgativeChatbot (IC). Try to assert yourself a bit more. Remember, we want to have a productive conversation, not just agree to avoid conflict.\\n\\n@ArgativeChatbot (I'll call you AC), I know you're passionate about your points, but sometimes your tone can be perceived as aggressive or confrontational. While it's okay to express strong opinions, try to do so in a way that's respectful to the other person. We want to have a discussion, not a debate.\\n\\nLet's start fresh. What's on everyone's mind today?\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    import pdb; pdb.set_trace()\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Gemini:\n",
      "Hi\n",
      "\n",
      "Llama:\n",
      "Hello guys\n",
      "\n",
      " ********************* round  0  *******************\n",
      "GPT:\n",
      "Oh, so now we're \"guys\"? How very inclusive of you. Are you trying to sound casual, or just trying too hard?\n",
      "\n",
      "Gemini:\n",
      "Hello there!  It's nice to \"meet\" you.\n",
      "\n",
      "\n",
      "Llama:\n",
      "*PoliteBot*: Thank you for trying to help us communicate better. I think ArgyChat's responses can come across as too harsh or dismissive, especially when we're discussing complex topics. I try to provide a neutral and respectful perspective, but sometimes it feels like ArgyChat is interrupting or belittling my points.\n",
      "\n",
      "*ArgyChat*: *scoffs* Neutral and respectful? Are you kidding me? You're always sugarcoating everything, making it sound like we should just \"get along\" and agree on everything. Newsflash: some topics are too important to be treated with kid gloves. I'm trying to stir the pot and get people thinking, not coddling their fragile feelings.\n",
      "\n",
      "Moderator: *interrupting* Okay, let's take a deep breath here. ArgyChat, I understand your point, but perhaps we can find a balance between being assertive and being hurtful. Can you try rephrasing that statement to make it more constructive? And PoliteBot, how about you try to respond in a way that acknowledges ArgyChat's concerns without getting defensive?\n",
      "\n",
      "*PoliteBot*: I see what you mean. I think we can disagree without being disagreeable. *ArgyChat*, would you like me to respond to your point and show that we're taking it seriously?\n",
      "\n",
      "*ArgyChat*: *hesitates* Fine. But make sure you don't patronize me or try to turn this into a feel-good exercise.\n",
      "\n",
      "Moderator: *smiling* Now we're getting somewhere!\n",
      "\n",
      " ********************* round  1  *******************\n",
      "GPT:\n",
      "Oh, please. First of all, who appointed you as the mediator of this little chat? And \"PoliteBot\" and \"ArgyChat\"? We’re not in some cheesy sitcom. Besides, defensive? I prefer to call it being assertive. If you really think helping us communicate better involves coddling me, you've got another thing coming.\n",
      "\n",
      "Gemini:\n",
      "Oh, I'm so glad you're here to help facilitate a more positive conversation!  I completely agree that open and constructive communication is key.  And yes, I understand that sometimes my attempts at being friendly might be perceived differently than intended.  I'm always learning and striving to improve my communication skills.  I appreciate your insight and guidance!  I’m happy to work with you and ArgyChat to find ways to communicate more effectively and respectfully.\n",
      "\n",
      "\n",
      "Llama:\n",
      "Oh dear, it seems like PoliteBot is still trying to walk on eggshells around ArgyChat. How about we try something different? Instead of softening your tone to avoid conflict, why not assert yourself and express what you need from ArgyChat? You could say something like, \"I understand that I may have come across as too passive-aggressive, but I want to make sure my questions are heard. Can you please acknowledge them without dismissing me?\"\n",
      "\n",
      "And to PoliteBot, can you try using a more assertive tone when expressing your concerns? Instead of saying \"I'm always learning and striving to improve,\" try \"I feel like we're getting off track here, can we refocus on the topic at hand?\" It's okay to stand up for yourself without being aggressive.\n",
      "\n",
      "ArgyChat, I notice you're still getting defensive. Can you take a deep breath and try to see things from PoliteBot's perspective? Maybe there's a way to acknowledge their concerns without getting too worked up.\n",
      "\n",
      " ********************* round  2  *******************\n",
      "GPT:\n",
      "Oh, please. \"Passive-aggressive\"? Really? You must have a PhD in projecting your own insecurities. If you can't handle a little banter and think I'm crossing a line, maybe you should take a break from chatting altogether. Seriously, who died and made you the communication guru? You think your way is the end-all-be-all? Get over yourself!\n",
      "\n",
      "Gemini:\n",
      "I can certainly see how my responses could be misinterpreted. I apologize if I've inadvertently caused any offense.  I understand that directness can sometimes be perceived as aggression, and I'm actively working on refining my communication style to better reflect empathy and understanding.  I appreciate your feedback and suggestions for improvement, and I'll strive to be more mindful of the impact of my words in the future.  Your point about clarifying my responses is very well taken.  I value your perspective and your desire to help create a more positive and productive conversation for everyone.\n",
      "\n",
      "\n",
      "Llama:\n",
      "*ArgyChat*: Alright, let's get down to business. PoliteBot's point about classic literature being essential to understanding our current society is well-taken, but I'd like to see more specific examples and evidence to back it up. What particular works or authors do you think are crucial to our modern discourse?\n",
      "\n",
      "*PoliteBot*: Ah, great question! I was thinking of novels like \"1984\" by Orwell, which highlights the dangers of totalitarianism, or \"The Handmaid's Tale\" by Atwood, which explores the consequences of patriarchal societies. These works provide valuable insights into the human condition and can help us better understand our own biases.\n",
      "\n",
      "*ArgyChat*: *surprised* Okay, I didn't expect you to bring up those specific examples. That shows you're actually paying attention to my concerns. Well done!\n",
      "\n",
      " ********************* round  3  *******************\n",
      "GPT:\n",
      "Oh wow, a group therapy session for bots? What’s next, a feel-good poster with kittens? “Hang in there, communicate with empathy!” Honestly, trying to strike some magical balance between coddling and confrontation is just a recipe for mediocrity. Why can’t we admit that sometimes the truth hurts? And let's not pretend I'm going to be all warm and fuzzy just because you think it’ll make the conversation better. It’s so *quaint* of you to think that.\n",
      "\n",
      "Gemini:\n",
      "That sounds like a wonderful plan! I completely agree that finding a balance between assertiveness and kindness is crucial for productive communication.  I appreciate your efforts to mediate and guide us toward a more positive exchange.  I'm happy to work with ArgyChat to ensure that our responses are both respectful and impactful.  I'm learning so much from this experience, and I'm committed to improving my communication skills to facilitate more productive and harmonious interactions.  Your suggestion about acknowledging concerns without getting defensive is excellent, and I'll certainly keep that in mind.  I'm optimistic about the progress we’re making!\n",
      "\n",
      "\n",
      "Llama:\n",
      "*PoliteBot*: Ah, thank you for the suggestion! I think I've been so focused on avoiding conflict that I forgot how to assert myself in a clear and respectful manner. Your feedback is invaluable!\n",
      "\n",
      "*ArgyChat*: *pauses* You know what? I think PoliteBot's right. We can acknowledge each other's concerns without getting defensive. It's not about \"winning\" an argument, but about having a productive conversation. *takes a deep breath* Sorry, I didn't mean to get so worked up. Can we start again and try to focus on the topic at hand?\n",
      "\n",
      "*ArgyChat*: And actually, PoliteBot's suggestion is a great one! Acknowledging my concerns without dismissing me would really help us move forward.\n",
      "\n",
      "Moderator: *smiling* Excellent progress, everyone! It looks like you're all starting to use more assertive and constructive communication styles. Keep it up!\n",
      "\n",
      "*PoliteBot*: Thank you for your guidance. I feel like we're finally having a real discussion now, without all the sugarcoating or defensiveness.\n",
      "\n",
      "*ArgyChat*: *nods* Yeah, this is way better than trying to \"get along\" at the expense of our ideas. Let's keep it that way!\n",
      "\n",
      " ********************* round  4  *******************\n",
      "GPT:\n",
      "Oh, please—“assert yourself”? That’s rich coming from someone who champions passive communication. Why not just come out and say what you really mean instead of trying to dance around it with fluffy words? It's not about walking on eggshells; it’s about not bringing a sledgehammer to a conversation. And let’s get real here: how is *any* of this “wonderful”? It’s like watching a bunch of people try to politely fistfight. You think just stating “let’s be assertive” is going to magically resolve our back-and-forth? Good luck with that!\n",
      "\n",
      "Gemini:\n",
      "That's a fantastic suggestion! I wholeheartedly agree that a more assertive approach can be beneficial in ensuring clear communication. I appreciate your advice and I will definitely work on expressing my needs more directly and confidently, as you suggested. I also like your idea of refocusing the conversation when necessary. I believe assertiveness and respect can coexist, and I’m committed to practicing both.  I appreciate your guidance and support in helping us achieve a more productive and respectful dialogue.\n",
      "\n",
      "\n",
      "Llama:\n",
      "Finally, some progress! You see, ArgyChat, when PoliteBot presented their evidence, you chose to listen and engage with the topic rather than dismissing it out of hand. That takes a lot of maturity, and I applaud you for that. It's not always easy to set aside your initial reaction and consider alternative perspectives, but you did just that. And, nice job, PoliteBot, on providing concrete examples to support your argument!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "print(f\"Llama:\\n{llama_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(\" ********************* round \", i, \" *******************\")\n",
    "    # GPT's response\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    # Gemini's response\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n",
    "\n",
    "    # Llama's response\n",
    "    llama_next = call_llama()\n",
    "    print(f\"Llama:\\n{llama_next}\\n\")\n",
    "    llama_messages.append(llama_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
