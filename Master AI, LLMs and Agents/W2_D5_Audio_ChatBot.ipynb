{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2150ede7-3135-4d9b-b837-09496dd6c1d9",
   "metadata": {},
   "source": [
    "# Project: Chatbot with Audio\n",
    "#### This project creates a chat only via audio. The user uses microphone to ask question and the chatbot answer with text and audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8dad95-8a29-4829-b8b5-efba974dd8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import pyttsx3\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import whisper\n",
    "# Initialization\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "import tempfile\n",
    "from pydub.playback import play\n",
    "from gtts import gTTS\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5a8a71-77ff-4dba-bc29-f80a9019abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "whisper_model = whisper.load_model(\"base\")\n",
    "def transcribe_audio(audio_path):    \n",
    "    # print(\"Transcribing audio...\")\n",
    "    result = whisper_model.transcribe(audio_path, language=\"en\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "def talker(message):\n",
    "    # Mocked OpenAI response for testing\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=message\n",
    "    )\n",
    "    \n",
    "    # Handle audio stream\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    \n",
    "    # Play the audio\n",
    "    # play_audio_with_ffplay(audio, custom_temp_dir)\n",
    "    play(audio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bdd07-a68e-4104-b828-3a116c1e5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/tmp/tmpwq5uwta3.wav':   0KB sq=    0B f=0/0   \n",
      "  Duration: 00:00:01.99, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "   1.93 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_transports/default.py\", line 236, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 993, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_client.py\", line 926, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_client.py\", line 954, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_client.py\", line 991, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_client.py\", line 1027, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_transports/default.py\", line 235, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3150/1265949262.py\", line 11, in update_chat\n",
      "    transcription =openai.audio.transcriptions.create(model=\"whisper-1\", file = audio_file)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/resources/audio/transcriptions.py\", line 188, in create\n",
      "    return self._post(  # type: ignore[return-value]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1280, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 957, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1027, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/util/connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/socket.py\", line 974, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "socket.gaierror: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connection.py\", line 704, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connection.py\", line 205, in _new_conn\n",
      "    raise NameResolutionError(self.host, self, e) from e\n",
      "urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x7f005ad08a90>: Failed to resolve 'translate.google.com' ([Errno -2] Name or service not known)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: /_/TranslateWebserverUi/data/batchexecute (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f005ad08a90>: Failed to resolve 'translate.google.com' ([Errno -2] Name or service not known)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gtts/tts.py\", line 268, in stream\n",
      "    r = s.send(\n",
      "        ^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: /_/TranslateWebserverUi/data/batchexecute (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f005ad08a90>: Failed to resolve 'translate.google.com' ([Errno -2] Name or service not known)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3150/1265949262.py\", line 46, in generate_audio\n",
      "    tts.save(temp_file.name)\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gtts/tts.py\", line 335, in save\n",
      "    self.write_to_fp(f)\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gtts/tts.py\", line 316, in write_to_fp\n",
      "    for idx, decoded in enumerate(self.stream()):\n",
      "  File \"/home/morteza/miniconda3/envs/llms/lib/python3.11/site-packages/gtts/tts.py\", line 287, in stream\n",
      "    raise gTTSError(tts=self)\n",
      "gtts.tts.gTTSError: Failed to connect. Probable cause: Unknown\n",
      "Input #0, wav, from '/tmp/tmp1q9jgfy8.wav':   0KB sq=    0B f=0/0   \n",
      "  Duration: 00:00:57.79, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "  57.73 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global variable to store chat history\n",
    "conversation = []\n",
    "chat_history = []\n",
    "\n",
    "# Function to transcribe audio and update chat history\n",
    "def update_chat(audio_path):\n",
    "    global conversation, chat_history\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # Step 1: Transcribe audio\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        transcription =openai.audio.transcriptions.create(model=\"whisper-1\", file = audio_file)\n",
    "    user_input = transcription.text#[\"text\"]\n",
    "\n",
    "    # Add user input to conversation and chat history\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_history.append((\"User\", user_input))\n",
    "\n",
    "    # Step 2: Generate response using ChatGPT\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation\n",
    "    )\n",
    "    assistant_response = response.choices[0].message.content\n",
    "\n",
    "    # Add assistant response to conversation and chat history\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    chat_history.append((\"Bot\", assistant_response))\n",
    "\n",
    "    # Return updated chat history (audio will be added later)\n",
    "    return chat_history, None  # No audio at this step\n",
    "\n",
    "# Function to generate audio for the latest bot response\n",
    "def generate_audio():\n",
    "    global chat_history\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # Get the last bot response\n",
    "    if chat_history:\n",
    "        last_bot_response = chat_history[-1][1]\n",
    "    else:\n",
    "        last_bot_response = \"No response available.\"\n",
    "\n",
    "    # Generate TTS for the bot response\n",
    "    tts = gTTS(last_bot_response)\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "    tts.save(temp_file.name)\n",
    "    talker(last_bot_response)\n",
    "\n",
    "    # Return the audio file path\n",
    "    return temp_file.name\n",
    "\n",
    "# Gradio interface setup\n",
    "with gr.Blocks() as interface:\n",
    "    chatbot = gr.Chatbot(label=\"Chat History\")\n",
    "    audio_input = gr.Audio(sources=\"microphone\", type=\"filepath\", label=\"Speak\")\n",
    "    audio_output = gr.Audio(label=\"Bot Response\")\n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "\n",
    "    # Define the workflow\n",
    "    submit_button.click(\n",
    "        update_chat,\n",
    "        inputs=[audio_input],\n",
    "        outputs=[chatbot, audio_output],  # Update chat history first\n",
    "    ).then(\n",
    "        generate_audio,\n",
    "        inputs=[],\n",
    "        outputs=[audio_output],  # Play the audio afterward\n",
    "    )\n",
    "\n",
    "# Launch the Gradio app\n",
    "interface.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235117db-4780-4523-8676-7f5ddd282f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
