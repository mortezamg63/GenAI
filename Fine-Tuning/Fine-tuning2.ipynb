{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/cs_mmoha014/.cache/huggingface/token\n",
      "Login successful\n",
      "logged in successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # This silences all warnings\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "print(\"logged in successfully\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"databricks/dolly-v2-3b\"#\"TheBloke/Llama-2-7B-AWQ\"#\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "#MODEL_NAME=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = torch.device(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', trust_remote_code=False, revision='main')\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "comment=\"great content, thank you!\"\n",
    "prompt=f'''[INST]{comment}[/INST]'''\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]great content, thank you![/INST]\n",
      "\n",
      "[INST]I'm glad you enjoyed it![/INST]\n",
      "\n",
      "[INST]Please let me know if you have any comments or questions about the video! I'd love to hear what you thought![/INST]\n",
      "\n",
      "[INST]Thank you! I really appreciate your feedback![/INST]\n",
      "\n",
      "[INST]You're welcome! I hope you enjoy the rest of the video![/INST]\n",
      "\n",
      "[INST]I sure will![/INST]\n",
      "\n",
      "[INST]Thank you! I'm looking forward to it![/INST]\n",
      "\n",
      "[INST]You're welcome! Have a nice day![/INST]\n",
      "\n",
      "[INST]You too\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids=inputs['input_ids'], max_new_tokens=140)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check prompt engineering in together.ai\n",
    "\n",
    "We can try different prompts to find a good one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine-Tuning\n",
    "model.train() # model in training mode (dropout modules are activated)\n",
    "model.gradient_checkpointing_enable() # enable gradient check pointing\n",
    "model = prepare_model_for_kbit_training(model) # enable quantized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,908,480 || all params: 2,785,994,560 || trainable%: 0.3915\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"dense_h_to_4h\", \"dense_4h_to_h\", \"query_key_value\", \"dense\", \"embed_out\",], lora_dropout=0.05, bias='none', task_type='CAUSAL_LM')\n",
    "model = get_peft_model(model, config)\n",
    "# LoRA trainable parameter count\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- Preprocess Text --------------\n",
    "from datasets import Dataset, load_dataset\n",
    "#load dataset\n",
    "data = load_dataset(\"shawhin/shawgpt-youtube-comments\")\n",
    "#https://github.com/shwhint/youtube-blog/tree/main/LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(examples):\n",
    "   # extract text\n",
    "   text = examples['example']\n",
    "   #tokenize and truncate text\n",
    "   tokenizer.truncation_side='left'\n",
    "   tokenized_inputs = tokenizer(text, return_tensors='np', truncation=True, max_length=512)\n",
    "   return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have a lot of data (the loaded dataset has only 60 samples) and each sample in dataset can have different length, we need to add a padding token to the end of shorter samples to make even length for all samples. For this purpose we can use <u><i>data collator </i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a396c324ce546c7bf68ea3cecadfd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7d72253c984392987f484a1ce8a2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "# -----setting pad token -----\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ----- data collator -----\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=masked language modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lr=2e-4\n",
    "batch_size=4\n",
    "num_epochs=10\n",
    "\n",
    "# define training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "   output_dir='tuned_model', learning_rate=lr, \n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   num_train_epochs=num_epochs,\n",
    "   weight_decay=0.01,\n",
    "   logging_strategy='epoch',\n",
    "   evaluation_strategy='epoch',\n",
    "   save_strategy='epoch',\n",
    "   load_best_model_at_end=True,\n",
    "   gradient_accumulation_steps=4,\n",
    "   warmup_steps=2,\n",
    "   fp16=True,\n",
    "   optim='paged_adamw_8bit') # optim is ingredients 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training/Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:01, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.249400</td>\n",
       "      <td>2.946505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.752700</td>\n",
       "      <td>1.971922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.664600</td>\n",
       "      <td>1.449531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.001800</td>\n",
       "      <td>1.356448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.232000</td>\n",
       "      <td>1.331331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.096600</td>\n",
       "      <td>1.328010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.048500</td>\n",
       "      <td>1.333788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.715100</td>\n",
       "      <td>1.346480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>1.355771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>1.357275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------- Run training/Fine-tuning ----------------\n",
    "trainer=transformers.Trainer(\n",
    "   model=model,\n",
    "   train_dataset=tokenized_data['train'],\n",
    "   eval_dataset = tokenized_data['test'],\n",
    "   args = training_args,\n",
    "   data_collator=data_collator)\n",
    "# train model\n",
    "model.config.use_cache=False # slience the warnings\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess the output text to have a good format \n",
    "using regular expression to remove unncessary symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_ins_text(pattern, text):\n",
    "  \"\"\"\n",
    "  Removes all occurrences of '[INST]'' or '[\\INST]' text from a string.\n",
    "\n",
    "  Args:\n",
    "      text: The string to process.\n",
    "\n",
    "  Returns:\n",
    "      The string with ['\\INS'] text removed.\n",
    "  \"\"\"\n",
    "  return re.sub(pattern, \"\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st generated response by the fined-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "# It reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. \\\n",
    "# ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "# thus keeping the interaction natural and engaging\n",
    "intstructions_string = f\"\"\".\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
    "\n",
    "comment = \"Great content, thank you!\"\n",
    "\n",
    "prompt = f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''#prompt_template(comment)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "\n",
    "# print(tokenizer.batch_decode(outputs)[0][:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please respond to the following comment. Great content, thank you! Glad to hear! \n"
     ]
    }
   ],
   "source": [
    "text =  tokenizer.batch_decode(outputs)[0]\n",
    "pattern = \"(\\[/INST(\\])*)|(\\[INST\\])|(\\</[pP]\\>)|(\\\\n)\"\n",
    "cleaned_text = remove_ins_text(pattern,text)\n",
    "print(cleaned_text[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd generated response by the fined-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "comment = \"What is fat tailedness?\"\n",
    "\n",
    "prompt = f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''#prompt_template(comment)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "# print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "What is fat tailedness? \n",
      "\n",
      "Fat tailedness is a term used to describe the shape of the probability density function (PDF) of the tail of a distribution. \n",
      "\n",
      "\n",
      "It is a property of the distribution that the probability of the data being further away from the mean than a certain value tends to zero as the value gets larger. \n",
      "\n",
      "\n",
      "For example, the normal distribution has a fat tailed distribution, as the normal distribution's PDF has a long tail, which means the probability of the data being further away from the mean than a certain value is not small. \n",
      "\n",
      "\n",
      "The term was coined in the field of statistics in the 1970s, and was used to describe the shape of the PDF of the data from the exponential distribution. \n",
      "\n",
      "\n",
      "The exponential distribution is a common choice for modeling the length of time it takes to observe a phenomenon, such as the duration of a computer program or the length of time it takes to observe a natural phenomenon. \n",
      "\n",
      "\n",
      "The exponential distribution has a fat tailed distribution, as the exponential distribution's PDF has a long tail, meaning the probability of the data being further away from the mean than a certain value is not small. \n",
      "\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "text =  tokenizer.batch_decode(outputs)[0]\n",
    "pattern = \"(\\[/INST\\])|(\\[INST\\])\"\n",
    "cleaned_text = remove_ins_text(pattern,text)\n",
    "print(cleaned_text[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
