{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d60dda-967e-49af-930f-78e22e176c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.11.0\n",
      "  Using cached datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/mmoha014/envs/subtab/lib/python3.7/site-packages (from datasets==2.11.0) (2023.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (2.25.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (1.3.5)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/mmoha014/envs/subtab/lib/python3.7/site-packages (from datasets==2.11.0) (0.16.4)\n",
      "Requirement already satisfied: xxhash in /home/mmoha014/envs/subtab/lib/python3.7/site-packages (from datasets==2.11.0) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (1.21.4)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.15-py37-none-any.whl (116 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (4.62.3)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-12.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.1 MB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==2.11.0) (4.11.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (4.1.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.11.0) (4.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.11.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets==2.11.0) (3.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.11.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.11.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.11.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.11.0) (2022.9.24)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==2.11.0) (3.6.0)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.11.0) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.11.0) (1.16.0)\n",
      "Installing collected packages: pyarrow, dill, responses, multiprocess, datasets\n",
      "Successfully installed datasets-2.11.0 dill-0.3.6 multiprocess-0.70.14 pyarrow-12.0.1 responses-0.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torchdata==0.5.1\n",
    "%pip install datasets==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ec61be-7d31-4a2d-bcbe-4dab77990a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca52d54-42bf-46f4-a8c6-db3b779b0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUMMARY_ID=1\n",
    "def transform_single_dialogsumm_file(file):\n",
    "    data = open(file,\"r\").readlines()\n",
    "    result = {\"fname\":[],\"summary\":[],\"dialogue\":[]}\n",
    "    for i in data:\n",
    "        d = json.loads(i)\n",
    "        for j in d.keys():\n",
    "            if j in result.keys():\n",
    "                result[j].append(d[j])\n",
    "    return Dataset.from_dict(result)\n",
    "\n",
    "def transform_test_file(file):\n",
    "    data = open(file,\"r\").readlines()\n",
    "    result = {\"fname\":[],\"summary%d\"%TEST_SUMMARY_ID:[],\"dialogue\":[]}\n",
    "    for i in data:\n",
    "        d = json.loads(i)\n",
    "        for j in d.keys():\n",
    "            if j in result.keys():\n",
    "                result[j].append(d[j])\n",
    "    \n",
    "    result[\"summary\"] = result[\"summary%d\"%TEST_SUMMARY_ID]\n",
    "    return Dataset.from_dict(result)\n",
    "\n",
    "def transform_dialogsumm_to_huggingface_dataset(train,validation,test):\n",
    "    train = transform_single_dialogsumm_file(train)\n",
    "    validation = transform_single_dialogsumm_file(validation)\n",
    "    test = transform_test_file(test)\n",
    "    return DatasetDict({\"train\":train,\"validation\":validation,\"test\":test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ad5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = transform_dialogsumm_to_huggingface_dataset(\"DialogSum_Data/dialogsum.train.jsonl\",\"DialogSum_Data/dialogsum.dev.jsonl\",\"DialogSum_Data/dialogsum.test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c1166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "***** Input Dialogue *****\n",
      "---------------------------------------------------------------------------------------------------\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "***** Human Summary *****\n",
      "---------------------------------------------------------------------------------------------------\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "***** Input Dialogue *****\n",
      "---------------------------------------------------------------------------------------------------\n",
      "#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "#Person2#: Well, depends. You see, there are eight famous Chinese food cuisines, for instance, Sichuan cuisine and Hunan cuisine.\n",
      "#Person1#: There're all spicy or hot of heard.\n",
      "#Person2#: That's right. If you have hot dishes, you can try some.\n",
      "#Person1#: I cannot have it. Last time I had some in the US. It almost killed me.\n",
      "#Person2#: And there are Cantonese and Kiangsu cuisines. Most people like them.\n",
      "#Person1#: Oh I'd like to try the Cantonese one. Where is it? Is it far?\n",
      "#Person2#: The one I know is about half an hour to go.\n",
      "#Person1#: Oh. That's too far away. I am really starvig. Do you have restaurant in your hotel?\n",
      "#Person2#: Oh sorry, we don't. But I know one nearby.\n",
      "#Person1#: What type?\n",
      "#Person2#: It's Beijing dishes. It's famous for the Beijing roast duck.\n",
      "#Person1#: OH, yes. I heard of a lot of about it. I like very much to try it. Where can I find it?\n",
      "#Person2#: The best place certainly is Quanjude restaurant.\n",
      "#Person1#: Is it near here?\n",
      "#Person2#: Yes, it takes fifteen minutes to walk there and five minutes to drive. If the traffic is not too bad, I mean.\n",
      "#Person1#: Well, thank you for your information. What's the name of that restaurant again?\n",
      "#Person2#: Let me write it down on a piece of paper for you. You can show to the taxi driver or ask for direction.\n",
      "#Person1#: That's very kind of you. Thanks a lot.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "***** Human Summary *****\n",
      "---------------------------------------------------------------------------------------------------\n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40,200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    \n",
    "    print('Example ', i+1)\n",
    "    print(dash_line)\n",
    "    print('***** Input Dialogue *****')\n",
    "    print(dash_line)\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('***** Human Summary *****')\n",
    "    print(dash_line)\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18deb408-1b25-44cf-8cf0-373ceffd9815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.038434505462646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 1404,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357b415056254c678e3f4a50299e6624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03483152389526367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 990345061,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b93eb5cd76f4a579b8b99dd481f4864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0700676441192627,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading generation_config.json",
       "rate": null,
       "total": 147,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b32cd3ad0924b87b59ffa000f210e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc74468-7f7e-46f0-9a09-bbc7956437b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03387570381164551,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 2537,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc2610debec44b3b2f2254b28924510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028215408325195312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading spiece.model",
       "rate": null,
       "total": 791656,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4decfe4854c40ca9b2b7345c6357ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03293871879577637,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 2424064,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac397fc19c84c54ae92cd2f1f830598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.032166481018066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 2201,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3899809c0164e5eb120350329e7b2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # Loading the tokernizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9162a7d6-1749-48b7-a6d7-48bb6b23b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded version:  tensor([ 125,   97,   19,   34,    6, 3059,   58,    1])\n",
      "decoded version:  what time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "# Convert a raw text of our conversation into the vector space. This vector space is usable in FlanT5 model\n",
    "sentence = \"what time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print (\"encoded version: \", sentence_encoded['input_ids'][0])\n",
    "print(\"decoded version: \", sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ced34-0f0a-43b4-98b1-274e8b349382",
   "metadata": {},
   "source": [
    "# Now we go for model with and without prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254f8dd-da85-4c35-ba3d-5c6b523909e5",
   "metadata": {},
   "source": [
    "### Without prompt engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d19481-4a80-4641-952b-4dd3324760f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model's summary:#Person1#: Thank you very much.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "#Person2#: Well, depends. You see, there are eight famous Chinese food cuisines, for instance, Sichuan cuisine and Hunan cuisine.\n",
      "#Person1#: There're all spicy or hot of heard.\n",
      "#Person2#: That's right. If you have hot dishes, you can try some.\n",
      "#Person1#: I cannot have it. Last time I had some in the US. It almost killed me.\n",
      "#Person2#: And there are Cantonese and Kiangsu cuisines. Most people like them.\n",
      "#Person1#: Oh I'd like to try the Cantonese one. Where is it? Is it far?\n",
      "#Person2#: The one I know is about half an hour to go.\n",
      "#Person1#: Oh. That's too far away. I am really starvig. Do you have restaurant in your hotel?\n",
      "#Person2#: Oh sorry, we don't. But I know one nearby.\n",
      "#Person1#: What type?\n",
      "#Person2#: It's Beijing dishes. It's famous for the Beijing roast duck.\n",
      "#Person1#: OH, yes. I heard of a lot of about it. I like very much to try it. Where can I find it?\n",
      "#Person2#: The best place certainly is Quanjude restaurant.\n",
      "#Person1#: Is it near here?\n",
      "#Person2#: Yes, it takes fifteen minutes to walk there and five minutes to drive. If the traffic is not too bad, I mean.\n",
      "#Person1#: Well, thank you for your information. What's the name of that restaurant again?\n",
      "#Person2#: Let me write it down on a piece of paper for you. You can show to the taxi driver or ask for direction.\n",
      "#Person1#: That's very kind of you. Thanks a lot.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model's summary:#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialog = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialog, return_tensors='pt')\n",
    "    model_out = model.generate(inputs['input_ids'], max_new_tokens=50,)\n",
    "    output = tokenizer.decode(model_out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'Input Prompt:\\n{dialog}')\n",
    "    print(dash_line)\n",
    "    print(f'Human Summary:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'Model\\'s summary:{output}')\n",
    "    print(dash_line)\n",
    "    print(dash_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061be536-628a-4dc2-97e0-a83fd871f525",
   "metadata": {},
   "source": [
    "### With prompt engineering - Zero Shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb69e81-63ae-4802-a5b6-8b567489dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model's summary:The pendant is broken.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "#Person2#: Well, depends. You see, there are eight famous Chinese food cuisines, for instance, Sichuan cuisine and Hunan cuisine.\n",
      "#Person1#: There're all spicy or hot of heard.\n",
      "#Person2#: That's right. If you have hot dishes, you can try some.\n",
      "#Person1#: I cannot have it. Last time I had some in the US. It almost killed me.\n",
      "#Person2#: And there are Cantonese and Kiangsu cuisines. Most people like them.\n",
      "#Person1#: Oh I'd like to try the Cantonese one. Where is it? Is it far?\n",
      "#Person2#: The one I know is about half an hour to go.\n",
      "#Person1#: Oh. That's too far away. I am really starvig. Do you have restaurant in your hotel?\n",
      "#Person2#: Oh sorry, we don't. But I know one nearby.\n",
      "#Person1#: What type?\n",
      "#Person2#: It's Beijing dishes. It's famous for the Beijing roast duck.\n",
      "#Person1#: OH, yes. I heard of a lot of about it. I like very much to try it. Where can I find it?\n",
      "#Person2#: The best place certainly is Quanjude restaurant.\n",
      "#Person1#: Is it near here?\n",
      "#Person2#: Yes, it takes fifteen minutes to walk there and five minutes to drive. If the traffic is not too bad, I mean.\n",
      "#Person1#: Well, thank you for your information. What's the name of that restaurant again?\n",
      "#Person2#: Let me write it down on a piece of paper for you. You can show to the taxi driver or ask for direction.\n",
      "#Person1#: That's very kind of you. Thanks a lot.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model's summary:#Person1#: Oh, I'm starving. I'd like to try some Chinese cuisine.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialog = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            Summarize the following conversation.\n",
    "            {dialog}\n",
    "            \n",
    "            Summary:\n",
    "            \"\"\"\n",
    "    # input to tokenizer is prompt variable instead of dialog variable\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    model_out = model.generate(inputs['input_ids'], max_new_tokens=50,)\n",
    "    output = tokenizer.decode(model_out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'Input Prompt:\\n{dialog}')\n",
    "    print(dash_line)\n",
    "    print(f'Human Summary:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'Model\\'s summary:{output}')\n",
    "    print(dash_line)\n",
    "    print(dash_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acab5a1-8686-40be-aebf-437ecb7e2cfd",
   "metadata": {},
   "source": [
    "Zero shot inference does not show much better summary. Let's try another prompt in zero shot inference to see the difference based on the prompt modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05715e0-dc2f-4090-b938-fe6405a77056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model's summary:The pendant is broken and Person1 wants to change it.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "#Person2#: Well, depends. You see, there are eight famous Chinese food cuisines, for instance, Sichuan cuisine and Hunan cuisine.\n",
      "#Person1#: There're all spicy or hot of heard.\n",
      "#Person2#: That's right. If you have hot dishes, you can try some.\n",
      "#Person1#: I cannot have it. Last time I had some in the US. It almost killed me.\n",
      "#Person2#: And there are Cantonese and Kiangsu cuisines. Most people like them.\n",
      "#Person1#: Oh I'd like to try the Cantonese one. Where is it? Is it far?\n",
      "#Person2#: The one I know is about half an hour to go.\n",
      "#Person1#: Oh. That's too far away. I am really starvig. Do you have restaurant in your hotel?\n",
      "#Person2#: Oh sorry, we don't. But I know one nearby.\n",
      "#Person1#: What type?\n",
      "#Person2#: It's Beijing dishes. It's famous for the Beijing roast duck.\n",
      "#Person1#: OH, yes. I heard of a lot of about it. I like very much to try it. Where can I find it?\n",
      "#Person2#: The best place certainly is Quanjude restaurant.\n",
      "#Person1#: Is it near here?\n",
      "#Person2#: Yes, it takes fifteen minutes to walk there and five minutes to drive. If the traffic is not too bad, I mean.\n",
      "#Person1#: Well, thank you for your information. What's the name of that restaurant again?\n",
      "#Person2#: Let me write it down on a piece of paper for you. You can show to the taxi driver or ask for direction.\n",
      "#Person1#: That's very kind of you. Thanks a lot.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model's summary:The first time to China, Person1 is looking for Chinese food. He's looking for a restaurant near his hotel. He's looking for a Cantonese restaurant. He's looking for a Cantonese\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialog = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            dialogue:\n",
    "            {dialog}\n",
    "            \n",
    "            What was going on?\n",
    "            \"\"\"\n",
    "    # input to tokenizer is prompt variable instead of dialog variable\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    model_out = model.generate(inputs['input_ids'], max_new_tokens=50,)\n",
    "    output = tokenizer.decode(model_out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'Input Prompt:\\n{dialog}')\n",
    "    print(dash_line)\n",
    "    print(f'Human Summary:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'Model\\'s summary:{output}')\n",
    "    print(dash_line)\n",
    "    print(dash_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13301123-8214-4009-a155-d95a2a54986e",
   "metadata": {},
   "source": [
    "## The model's summary is a little better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8593d7a-bb15-42ee-8fd6-65557442426e",
   "metadata": {},
   "source": [
    "# In-Context Learning via one shot or few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e24221af-e52c-4276-b47c-0cbf07d074aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialog = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequences\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "{dialog}\n",
    "\n",
    "What was going on? \n",
    "{summary}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "{dataset['test'][example_index_to_summarize]['dialogue']}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88f79b-31a2-43f6-a68c-804944ea5639",
   "metadata": {},
   "source": [
    "### Construct the prompt to perform inference\n",
    "# one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11845c8-1eec-4410-b097-e4dc1a434b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on? \n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "#Person2#: Well, depends. You see, there are eight famous Chinese food cuisines, for instance, Sichuan cuisine and Hunan cuisine.\n",
      "#Person1#: There're all spicy or hot of heard.\n",
      "#Person2#: That's right. If you have hot dishes, you can try some.\n",
      "#Person1#: I cannot have it. Last time I had some in the US. It almost killed me.\n",
      "#Person2#: And there are Cantonese and Kiangsu cuisines. Most people like them.\n",
      "#Person1#: Oh I'd like to try the Cantonese one. Where is it? Is it far?\n",
      "#Person2#: The one I know is about half an hour to go.\n",
      "#Person1#: Oh. That's too far away. I am really starvig. Do you have restaurant in your hotel?\n",
      "#Person2#: Oh sorry, we don't. But I know one nearby.\n",
      "#Person1#: What type?\n",
      "#Person2#: It's Beijing dishes. It's famous for the Beijing roast duck.\n",
      "#Person1#: OH, yes. I heard of a lot of about it. I like very much to try it. Where can I find it?\n",
      "#Person2#: The best place certainly is Quanjude restaurant.\n",
      "#Person1#: Is it near here?\n",
      "#Person2#: Yes, it takes fifteen minutes to walk there and five minutes to drive. If the traffic is not too bad, I mean.\n",
      "#Person1#: Well, thank you for your information. What's the name of that restaurant again?\n",
      "#Person2#: Let me write it down on a piece of paper for you. You can show to the taxi driver or ask for direction.\n",
      "#Person1#: That's very kind of you. Thanks a lot.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85bdc34d-271a-4602-a12f-e6c6f4d123c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary: \n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary: \n",
      "#Person1 is looking for a restaurant in Beijing. It's a Cantonese restaurant. It's near the hotel.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "model_out = model.generate(inputs['input_ids'], max_new_tokens=50,)\n",
    "output = tokenizer.decode(model_out[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print('Human Summary: ')\n",
    "print(summary)\n",
    "print(dash_line)\n",
    "print('Model Summary: ')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85589e6-1cfd-4533-9700-2082543a0668",
   "metadata": {},
   "source": [
    "### One shot learning made the summary a little bit better. So we can try few shot learning from now to see how it improves the model's performance\n",
    "\n",
    "# Few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "303258a4-7534-4d8a-8bda-2c15dacc9cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on? \n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Hello. Is this ABC Rent-a-car Company?\n",
      "#Person2#: Yes, speaking. May I help you?\n",
      "#Person1#: This morning we rented a car and we are on the way to Niagara Falls. I'm afraid we have a car accident near the border.\n",
      "#Person2#: That's too bad. What kind of accident is it? Are you all right?\n",
      "#Person1#: I'm all right, but my friend is seriously injured. Will you call an ambulance and the police?\n",
      "#Person2#: OK. I'll do it right away, but tell me how it happened.\n",
      "#Person1#: I ran into the guardrail when I turned to the left.\n",
      "\n",
      "What was going on? \n",
      "#Person1# rent a car from ABC Rent-a-car Company this morning and met an accident. #Person2# will call an ambulance and police for #Person1#.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you given the puppies food yet?\n",
      "#Person2#: Of course. I fed them today.\n",
      "#Person1#: Good job! Please give them a bath later on today.\n",
      "#Person2#: Sure, I'll give them a bath.\n",
      "#Person1#: Thanks. But don't forget that they have a vet appointment this Saturday.\n",
      "#Person2#: I remember. What time do they need to be there?\n",
      "#Person1#: They need to be there at eleven in the morning.\n",
      "#Person2#: All right. I'll make sure and remember.\n",
      "\n",
      "What was going on? \n",
      "#Person1# asks something about #Person2#'s care with puppies and reminds #Person2# of the vet appointment.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Oh, I'm starving. It's my first time to China. And I'd like to try some real Chinese cuisine. What would you recommend?\n",
      "#Person2#: Well, depends. You see, there are eight famous Chinese food cuisines, for instance, Sichuan cuisine and Hunan cuisine.\n",
      "#Person1#: There're all spicy or hot of heard.\n",
      "#Person2#: That's right. If you have hot dishes, you can try some.\n",
      "#Person1#: I cannot have it. Last time I had some in the US. It almost killed me.\n",
      "#Person2#: And there are Cantonese and Kiangsu cuisines. Most people like them.\n",
      "#Person1#: Oh I'd like to try the Cantonese one. Where is it? Is it far?\n",
      "#Person2#: The one I know is about half an hour to go.\n",
      "#Person1#: Oh. That's too far away. I am really starvig. Do you have restaurant in your hotel?\n",
      "#Person2#: Oh sorry, we don't. But I know one nearby.\n",
      "#Person1#: What type?\n",
      "#Person2#: It's Beijing dishes. It's famous for the Beijing roast duck.\n",
      "#Person1#: OH, yes. I heard of a lot of about it. I like very much to try it. Where can I find it?\n",
      "#Person2#: The best place certainly is Quanjude restaurant.\n",
      "#Person1#: Is it near here?\n",
      "#Person2#: Yes, it takes fifteen minutes to walk there and five minutes to drive. If the traffic is not too bad, I mean.\n",
      "#Person1#: Well, thank you for your information. What's the name of that restaurant again?\n",
      "#Person2#: Let me write it down on a piece of paper for you. You can show to the taxi driver or ask for direction.\n",
      "#Person1#: That's very kind of you. Thanks a lot.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40,80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf53ce98-dd73-4c2b-8be5-6e9ee3cc3afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary: \n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary: \n",
      "The first time to China, Person1 is looking for Chinese cuisine.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "model_out = model.generate(inputs['input_ids'], max_new_tokens=50,)\n",
    "output = tokenizer.decode(model_out[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print('Human Summary: ')\n",
    "print(summary)\n",
    "print(dash_line)\n",
    "print('Model Summary: ')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3871e04-9060-4d22-b732-b062dc826000",
   "metadata": {},
   "source": [
    "### We can see that few shot inference does not do better than one shot necessarily. \n",
    "Some people try to add more shots like 4, 5, 6 and more to improve the models performance, but the experience of teacher shows that more than 6 shots does not help much. Also, here you can see that the more than one shot inference does not help the model do better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cdbae-0dc1-4b2f-92f4-0f0159e47221",
   "metadata": {},
   "source": [
    "# Generative Configuration parameters for Inference\n",
    "We can change the configuration parameters to see a different output from the LLM.  So far, we only used max_new_tokens, which defines the maximum number of tokens to generate. A full list of available parameters can be foudn in the <a href=\"https://huggingface.co/docs/transformers/en/main_classes/text_generation\" >hugggingFace Generation Documentation</a>.\n",
    "\n",
    "A convenient way of organizing the configuration parameters is to use GenerationConfig class.\n",
    "\n",
    "We can, for example, put do_sample=True, activate various decoding strategies which influences the next token from the probability distribution over the entire vocabulary, adjust the output temperature and other parameters (such as top_k and top_p).\n",
    "\n",
    "<b>The parameter temperature</b>: closer to 0 give more conservative results, but closer to 1 gives more wild and bizzare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77469773-fc28-4ae9-8507-66a1df44006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary: \n",
      "It's #Person1#'s first time to China and #Person1# wants some Chinese cuisine. #Person2# recommends some but it's too far and #Person1# is starving. Then #Person2# suggests a nearby Quanjude restaurant and its Beijing roast duck. #Person1# will go there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary: \n",
      "The first time to China, Person1 is looking for Chinese cuisine.\n"
     ]
    }
   ],
   "source": [
    "# generation_config = GenerationConfig(max_new_tokens=50)\n",
    "generation_config = GenerationConfig(max_new_tokens=50,min_new_tokens=20)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True) # worse result\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1) #worse\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5) # No\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0) # No\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "model_out = model.generate(inputs['input_ids'], generation_config,)\n",
    "output = tokenizer.decode(model_out[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print('Human Summary: ')\n",
    "print(summary)\n",
    "print(dash_line)\n",
    "print('Model Summary: ')\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
